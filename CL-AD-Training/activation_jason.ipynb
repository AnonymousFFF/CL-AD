{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e7d2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\call\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\envs\\call\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# 定义与保存时一致的模型结构\n",
    "class ModifiedResNet18(torch.nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=False)  # 不使用预训练权重，因为我们加载的是已经训练好的模型\n",
    "        self.resnet18.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet18.maxpool = torch.nn.Identity()  # 移除最大池化层\n",
    "        \n",
    "        # 获取ResNet18的特征提取部分\n",
    "        self.features = torch.nn.Sequential(*list(self.resnet18.children())[:-1])\n",
    "        \n",
    "        # 添加MLP和全连接层\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(0.5)\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.mlp(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型\n",
    "model = ModifiedResNet18()\n",
    "\n",
    "# 加载保存的模型权重\n",
    "model_path = 'best_model.pth'  # 替换为你的模型文件路径\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# 如果你有GPU，可以将模型移动到GPU上\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 现在你可以使用这个模型进行推理或其他操作\n",
    "\n",
    "# 提取所有卷积层\n",
    "conv_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        conv_layers.append((name, module))\n",
    "\n",
    "# 定义从本地读取部分CIFAR-10数据的预处理\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # 返回图像和虚拟标签0\n",
    "\n",
    "def preprocess_local_cifar10(data_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataset = CustomDataset(data_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "# 在process函数中，将输入数据移动到模型所在的设备上\n",
    "def process(data_path, file_name):\n",
    "    # 替换为你的部分CIFAR-10本地数据路径\n",
    "    data_path = data_path\n",
    "    dataloader = preprocess_local_cifar10(data_path)\n",
    "\n",
    "    # 存储各层的激活值\n",
    "    activations = {}\n",
    "\n",
    "    # 定义钩子函数\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            # 记录每个卷积核的平均激活强度\n",
    "            activations[name] = output.detach().mean(dim=(0, 2, 3))  # [C] 维\n",
    "        return hook\n",
    "\n",
    "    # 为所有卷积层注册钩子\n",
    "    handles = []\n",
    "    for name, layer in conv_layers:\n",
    "        handle = layer.register_forward_hook(get_activation(name))\n",
    "        handles.append(handle)\n",
    "\n",
    "    # 统计所有部分CIFAR-10数据的激活情况\n",
    "    all_activation_stats = []\n",
    "\n",
    "    # 遍历所有批次\n",
    "    for batch_idx, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)  # 将输入数据移动到模型所在的设备上\n",
    "        with torch.no_grad():\n",
    "            model(inputs)\n",
    "\n",
    "        # 定义激活阈值（例如：全局平均值的50%）\n",
    "        all_activations = torch.cat([act.flatten() for act in activations.values()])\n",
    "        threshold = all_activations.mean().item() * 0.5  # 可调参数\n",
    "        threshold = 0\n",
    "\n",
    "        # 统计每个卷积层的激活情况\n",
    "        activation_stats = {}\n",
    "        for layer_name, act in activations.items():\n",
    "            activated_kernels = (act > threshold).nonzero().squeeze().tolist()\n",
    "            if isinstance(activated_kernels, int):\n",
    "                activated_kernels = [activated_kernels]\n",
    "            # 按照降序排列激活索引\n",
    "            activated_kernels.sort(reverse=True)\n",
    "            activation_stats[layer_name] = {\n",
    "                \"total_kernels\": len(act),\n",
    "                \"activated\": len(activated_kernels),\n",
    "                \"indices\": activated_kernels\n",
    "            }\n",
    "\n",
    "        all_activation_stats.append(activation_stats)\n",
    "\n",
    "    # 移除钩子\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    # 保存结果到本地文件\n",
    "    with open(file_name, \"w\") as f:\n",
    "        json.dump(all_activation_stats, f)\n",
    "\n",
    "    print(\"所有部分CIFAR-10数据的激活情况已保存到json 文件中\")\n",
    "\n",
    "    # 可视化部分（可选）\n",
    "    # 如果需要可视化，可以参考之前的代码，但需要注意数据量较大，可能需要采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d3e1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: resnet18.layer1.0.conv1, Index: 26, Total Activated: 5897, Total Kernels: 64\n",
      "Layer: resnet18.layer1.0.conv2, Index: 35, Total Activated: 5897, Total Kernels: 64\n",
      "Layer: resnet18.layer1.1.conv1, Index: 48, Total Activated: 5897, Total Kernels: 64\n",
      "Layer: resnet18.layer1.1.conv2, Index: 63, Total Activated: 5897, Total Kernels: 64\n",
      "Layer: resnet18.layer2.0.conv1, Index: 101, Total Activated: 5897, Total Kernels: 128\n",
      "Layer: resnet18.layer2.0.conv2, Index: 36, Total Activated: 5897, Total Kernels: 128\n",
      "Layer: resnet18.layer2.1.conv1, Index: 97, Total Activated: 5897, Total Kernels: 128\n",
      "Layer: resnet18.layer2.1.conv2, Index: 127, Total Activated: 5897, Total Kernels: 128\n",
      "Layer: resnet18.layer3.0.conv1, Index: 209, Total Activated: 5897, Total Kernels: 256\n",
      "Layer: resnet18.layer3.0.conv2, Index: 252, Total Activated: 5897, Total Kernels: 256\n",
      "Layer: resnet18.layer3.1.conv1, Index: 230, Total Activated: 5897, Total Kernels: 256\n",
      "Layer: resnet18.layer3.1.conv2, Index: 199, Total Activated: 5897, Total Kernels: 256\n",
      "Layer: resnet18.layer4.0.conv1, Index: 481, Total Activated: 5897, Total Kernels: 512\n",
      "Layer: resnet18.layer4.0.conv2, Index: 485, Total Activated: 5897, Total Kernels: 512\n",
      "Layer: resnet18.layer4.1.conv1, Index: 115, Total Activated: 5897, Total Kernels: 512\n",
      "Layer: resnet18.layer4.1.conv2, Index: 382, Total Activated: 5893, Total Kernels: 512\n",
      "Layer: resnet18.conv1, Index: 57, Total Activated: 5306, Total Kernels: 64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取 JSON 数据\n",
    "with open(r'D:\\python\\activate\\json\\cifar_airplane_activation.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 初始化字典以存储每层的节点激活统计信息\n",
    "layer_stats = {}\n",
    "\n",
    "# 遍历每个数据集（多个字典对象）\n",
    "for dataset in data:\n",
    "    # 遍历每个层及其激活信息\n",
    "    for layer_name, activation_info in dataset.items():\n",
    "        total_kernels = activation_info['total_kernels']\n",
    "        activated_indices = activation_info['indices']\n",
    "\n",
    "        # 如果层尚未存在于统计字典中，则创建一个新条目\n",
    "        if layer_name not in layer_stats:\n",
    "            layer_stats[layer_name] = {\n",
    "                \"total_kernels\": total_kernels,\n",
    "                \"index_counts\": {}\n",
    "            }\n",
    "\n",
    "        # 统计每个索引的激活次数\n",
    "        for index in activated_indices:\n",
    "            if index not in layer_stats[layer_name]['index_counts']:\n",
    "                layer_stats[layer_name]['index_counts'][index] = 0\n",
    "            layer_stats[layer_name]['index_counts'][index] += 1\n",
    "\n",
    "# 找出每个层中激活频率最高的节点\n",
    "highest_activated_layers = []\n",
    "for layer_name, stats in layer_stats.items():\n",
    "    # 跳过下采样层（downsample）\n",
    "    if 'downsample' in layer_name:\n",
    "        continue\n",
    "\n",
    "    # 获取激活次数最多的索引及其次数\n",
    "    if stats['index_counts']:\n",
    "        max_index = max(stats['index_counts'], key=stats['index_counts'].get)\n",
    "        max_count = stats['index_counts'][max_index]\n",
    "        highest_activated_layers.append({\n",
    "            \"layer_name\": layer_name,\n",
    "            \"index\": max_index,\n",
    "            \"total_activated\": max_count,\n",
    "            \"total_kernels\": stats['total_kernels']\n",
    "        })\n",
    "\n",
    "# 按照总激活次数从高到低排序\n",
    "highest_activated_layers.sort(key=lambda x: x['total_activated'], reverse=True)\n",
    "\n",
    "# 打印结果\n",
    "for info in highest_activated_layers:\n",
    "    print(f\"Layer: {info['layer_name']}, Index: {info['index']}, \"\n",
    "          f\"Total Activated: {info['total_activated']}, \"\n",
    "          f\"Total Kernels: {info['total_kernels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf1bf88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最高激活频率的层名：resnet18.layer1.0.conv1\n",
      "激活索引：26\n",
      "总激活次数：5897\n",
      "总激活节点数：64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取 JSON 数据\n",
    "with open(r'D:\\python\\activate\\json\\cifar_airplane_activation.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 初始化字典以存储每层的节点激活统计信息\n",
    "layer_stats = {}\n",
    "\n",
    "# 遍历每个数据集（多个字典对象）\n",
    "for dataset in data:\n",
    "    # 遍历每个层及其激活信息\n",
    "    for layer_name, activation_info in dataset.items():\n",
    "        total_kernels = activation_info['total_kernels']\n",
    "        activated_indices = activation_info['indices']\n",
    "\n",
    "        # 如果层尚未存在于统计字典中，则创建一个新条目\n",
    "        if layer_name not in layer_stats:\n",
    "            layer_stats[layer_name] = {\n",
    "                \"total_kernels\": total_kernels,\n",
    "                \"index_counts\": {}\n",
    "            }\n",
    "\n",
    "        # 统计每个索引的激活次数\n",
    "        for index in activated_indices:\n",
    "            if index not in layer_stats[layer_name]['index_counts']:\n",
    "                layer_stats[layer_name]['index_counts'][index] = 0\n",
    "            layer_stats[layer_name]['index_counts'][index] += 1\n",
    "\n",
    "# 找出每个层中激活频率最高的节点\n",
    "highest_activated_layers = []\n",
    "for layer_name, stats in layer_stats.items():\n",
    "    # 跳过下采样层（downsample）\n",
    "    if 'downsample' in layer_name:\n",
    "        continue\n",
    "\n",
    "    # 获取激活次数最多的索引及其次数\n",
    "    if stats['index_counts']:\n",
    "        max_index = max(stats['index_counts'], key=stats['index_counts'].get)\n",
    "        max_count = stats['index_counts'][max_index]\n",
    "        highest_activated_layers.append({\n",
    "            \"layer_name\": layer_name,\n",
    "            \"index\": max_index,\n",
    "            \"total_activated\": max_count,\n",
    "            \"total_kernels\": stats['total_kernels']\n",
    "        })\n",
    "\n",
    "# 按照总激活次数从高到低排序\n",
    "highest_activated_layers.sort(key=lambda x: (x['total_activated'], -x['index']), reverse=True)\n",
    "\n",
    "# 找出激活索引总体最靠前的一个节点\n",
    "max_total_activated = highest_activated_layers[0]['total_activated']\n",
    "closest_to_start = None\n",
    "\n",
    "for info in highest_activated_layers:\n",
    "    if info['total_activated'] == max_total_activated:\n",
    "        if closest_to_start is None or info['index'] < closest_to_start['index']:\n",
    "            closest_to_start = info\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 打印结果\n",
    "if closest_to_start:\n",
    "    print(f\"最高激活频率的层名：{closest_to_start['layer_name']}\")\n",
    "    print(f\"激活索引：{closest_to_start['index']}\")\n",
    "    print(f\"总激活次数：{closest_to_start['total_activated']}\")\n",
    "    print(f\"总激活节点数：{closest_to_start['total_kernels']}\")\n",
    "else:\n",
    "    print(\"未找到符合条件的节点\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a5c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57dba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740e750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202aa1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe30b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67115d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5471928",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m path_list:\n\u001b[0;32m     13\u001b[0m     filename\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_activation.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 121\u001b[0m, in \u001b[0;36mprocess\u001b[1;34m(data_path, file_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 将输入数据移动到模型所在的设备上\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 121\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# 定义激活阈值（例如：全局平均值的50%）\u001b[39;00m\n\u001b[0;32m    124\u001b[0m all_activations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([act\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m activations\u001b[38;5;241m.\u001b[39mvalues()])\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m, in \u001b[0;36mModifiedResNet18.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torchvision\\models\\resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1212\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\call\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_list=[r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_airplane\",\n",
    "           r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_automobile\",\n",
    "          r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_bird\",\n",
    "          r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_cat\",\n",
    "          r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_deer\",\n",
    "          r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_dog\",\n",
    "          r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_frog\",\n",
    "          r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_horse\",\n",
    "          r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_ship\",\n",
    "          r\"D:\\python\\resnetcifar\\cifar10_sorted\\cifar_truck\"]\n",
    "\n",
    "for i in path_list:\n",
    "    filename=i.split(\"_\")[-1]+\"_activation.json\"\n",
    "    process(i,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242ebfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for batch_idx, activation_stats in enumerate(all_activation_stats):\\n    print(f\"批次 {batch_idx + 1}:\")\\n    for layer_name, stats in activation_stats.items():\\n        print(f\"  层名: {layer_name}\")\\n        print(f\"    总卷积核数: {stats[\\'total_kernels\\']}\")\\n        print(f\"    激活的卷积核数: {stats[\\'activated\\']}\")\\n        print(f\"    激活的卷积核索引: {stats[\\'indices\\'][:5]}...\")  # 只显示前5个索引\\n    print(\"-\" * 50)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(\"airplane_activation.json\", \"r\") as f:\n",
    "    all_activation_stats = json.load(f)\n",
    "\n",
    "# 显示激活统计信息\n",
    "\"\"\"for batch_idx, activation_stats in enumerate(all_activation_stats):\n",
    "    print(f\"批次 {batch_idx + 1}:\")\n",
    "    for layer_name, stats in activation_stats.items():\n",
    "        print(f\"  层名: {layer_name}\")\n",
    "        print(f\"    总卷积核数: {stats['total_kernels']}\")\n",
    "        print(f\"    激活的卷积核数: {stats['activated']}\")\n",
    "        print(f\"    激活的卷积核索引: {stats['indices'][:5]}...\")  # 只显示前5个索引\n",
    "    print(\"-\" * 50)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09be747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "def find_most_frequent_kernel(json_file):\n",
    "    # 读取JSON文件\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    counts = defaultdict(lambda: defaultdict(int))  # 统计各层各索引的激活次数\n",
    "    \n",
    "    # 遍历每个图像的数据\n",
    "    for image_data in data:\n",
    "        for layer_name, stats in image_data.items():\n",
    "            indices = stats['indices']\n",
    "            for idx in indices:\n",
    "                counts[layer_name][idx] += 1\n",
    "    \n",
    "    # 找出每个层的最高频率索引\n",
    "    max_per_layer = {}\n",
    "    for layer_name, indices in counts.items():\n",
    "        if not indices:\n",
    "            continue\n",
    "        max_idx = max(indices, key=lambda k: indices[k])\n",
    "        max_count = indices[max_idx]\n",
    "        max_per_layer[layer_name] = (max_idx, max_count)\n",
    "    \n",
    "    # 找出全局最高\n",
    "    global_max_layer = None\n",
    "    global_max_idx = -1\n",
    "    global_max_count = 0\n",
    "    for layer_name, (idx, count) in max_per_layer.items():\n",
    "        if count > global_max_count:\n",
    "            global_max_count = count\n",
    "            global_max_layer = layer_name\n",
    "            global_max_idx = idx\n",
    "    \n",
    "    return global_max_layer, global_max_idx, global_max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaab9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别: airplane\n",
      "  最高激活层: resnet18.layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5897\n",
      "\n",
      "类别: automobile\n",
      "  最高激活层: resnet18.layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5913\n",
      "\n",
      "类别: bird\n",
      "  最高激活层: resnet18.layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5891\n",
      "\n",
      "类别: cat\n",
      "  最高激活层: resnet18.layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5900\n",
      "\n",
      "类别: deer\n",
      "  最高激活层: layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5887\n",
      "\n",
      "类别: dog\n",
      "  最高激活层: layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5893\n",
      "\n",
      "类别: frog\n",
      "  最高激活层: layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5906\n",
      "\n",
      "类别: horse\n",
      "  最高激活层: layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5898\n",
      "\n",
      "类别: ship\n",
      "  最高激活层: layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5909\n",
      "\n",
      "类别: truck\n",
      "  最高激活层: layer1.0.conv1\n",
      "  卷积核索引: 62\n",
      "  激活次数: 5900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "             'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "results = []\n",
    "for category in categories:\n",
    "    json_file = f\"{category}_activation.json\"\n",
    "    try:\n",
    "        layer, idx, count = find_most_frequent_kernel(json_file)\n",
    "        results.append({\n",
    "            '类别': category,\n",
    "            '最高激活层': layer,\n",
    "            '卷积核索引': idx,\n",
    "            '激活次数': count\n",
    "        })\n",
    "    except FileNotFoundError:\n",
    "        print(f\"文件 {json_file} 不存在，跳过\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"文件 {json_file} 格式错误，跳过\")\n",
    "\n",
    "# 打印结果\n",
    "for res in results:\n",
    "    print(f\"类别: {res['类别']}\")\n",
    "    print(f\"  最高激活层: {res['最高激活层']}\")\n",
    "    print(f\"  卷积核索引: {res['卷积核索引']}\")\n",
    "    print(f\"  激活次数: {res['激活次数']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8916695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_index(model, layer_name):\n",
    "    for idx, layer in enumerate(model.modules()):\n",
    "        if layer.__class__.__name__ == 'Sequential':\n",
    "            for sub_layer in layer.modules():\n",
    "                if sub_layer.__class__.__name__ != 'Sequential' and sub_layer.__class__.__name__ != 'ModuleList' and sub_layer.__class__.__name__ != 'ModuleDict':\n",
    "                    if sub_layer.__class__.__name__ == layer_name.split('.')[-1]:\n",
    "                        return idx\n",
    "        else:\n",
    "            if layer.__class__.__name__ == layer_name.split('.')[-1]:\n",
    "                return idx\n",
    "    return -1  # 如果找不到层，返回-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de24cd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer index of 'layer1.0': -1\n"
     ]
    }
   ],
   "source": [
    "layer_name = \"layer1.0.conv1\"\n",
    "idx = get_layer_index(model, layer_name)\n",
    "print(f\"Layer index of '{layer_name}':\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d4290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
